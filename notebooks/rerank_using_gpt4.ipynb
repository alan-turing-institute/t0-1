{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53225c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename= \"../data/evaluation/evaluation_vector_store_results_2025-04-26_12-03-28.jsonl\" #k=30\n",
    "#filename = \"../data/evaluation/evaluation_vector_store_results_2025-04-26_12-11-06.jsonl\" #k=50\n",
    "with open(filename, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_performance = sum([x[\"match\"] for x in data])/len(data)\n",
    "print(f\"Retriever performance: {retriever_performance}\")\n",
    "\n",
    "query_types = set([x[\"query_type\"] for x in data])\n",
    "for query_type in query_types:\n",
    "    query_type_data = [x for x in data if x[\"query_type\"] == query_type]\n",
    "    retriever_performance = sum([x[\"match\"] for x in query_type_data])/len(query_type_data)\n",
    "    print(f\"Retriever performance for {query_type}: {retriever_performance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da126efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Specify the path to your .env file\n",
    "dotenv_path = Path(\"../.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "def get_env_var(key: str) -> str:\n",
    "    try:\n",
    "        return os.environ[key]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Please set the {key} environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d480b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(symptoms_description, document_titles, document_text, k):\n",
    "    prompt = f\"\"\"\n",
    "    You are part of a retrieval system for a medical domain.\n",
    "    Given a description of symptoms provided by a patient, an initial retriever has shortlisted several possible conditions, along with the distance score of the most relevant snippet for that condition (so lower is better), the number of snippets retrieved and the entire content of the associated document.\n",
    "    \n",
    "    Here is the patient's symptom description:\n",
    "    {symptoms_description}\n",
    "    \n",
    "    The shortlisted conditions and their retrieval scores and number of snippets are:\n",
    "    {document_titles}\n",
    "    \n",
    "    The corresponding condition descriptions are:\n",
    "    {document_text}\n",
    "    \n",
    "    Your task is to select the {k} most likely conditions based on the symptoms. \n",
    "    Please return only the titles of the selected conditions, comma-separated.\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "# basic function to prompt the model\n",
    "def get_response_ollama(prompt, model=\"gemma3:1b\"):\n",
    "    response = ollama.generate(model=model, prompt=prompt)\n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_TOKENS = 2048\n",
    "AZURE_OPENAI_API_VERSION = \"2024-12-01-preview\"\n",
    "\n",
    "endpoint = get_env_var(\"AZURE_OPENAI_ENDPOINT_gpt-4o\")\n",
    "key = get_env_var(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "gpt_4o_client = AzureOpenAI(\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=key,\n",
    "        )\n",
    "\n",
    "def get_response_gpt4(prompt: str, model: str) -> str:\n",
    "    response = gpt_4o_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        max_completion_tokens=MAX_TOKENS,\n",
    "        model=model,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_response_gpt4(\"test\", \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client\n",
    "qwen_client = OpenAI(\n",
    "    api_key=\"EMPTY\",  # vLLM doesn't require an API key\n",
    "    base_url=\"http://localhost:8000/v1\"\n",
    ")\n",
    "\n",
    "def get_response_qwen(prompt: str, model: str) -> str:\n",
    "    response = qwen_client.completions.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/nhs-conditions/v3/conditions.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "conditions = {}\n",
    "for line in lines:\n",
    "    condition = json.loads(line)\n",
    "    conditions[condition[\"condition_title\"]] = condition['condition_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fa4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_rank_documents(line, k):\n",
    "    symptoms_description = line['symptoms_description']\n",
    "    # document_titles now contains tuples of (source, score)\n",
    "    document_titles = [(source, score) for source, score in zip(line[\"retrieved_documents_sources\"], line[\"retrieved_documents_scores\"])]\n",
    "\n",
    "    # Initialize a dictionary to store best sources with additional data\n",
    "    best_sources = {}\n",
    "\n",
    "    for source, score in document_titles:\n",
    "        # If the source is not in best_sources or the current score is lower than the stored one\n",
    "        if source not in best_sources:\n",
    "            best_sources[source] = {'lowest_score': score, 'number_of_snippets': 1}  # Initialize the source with a count of 1 snippet\n",
    "        else:\n",
    "            best_sources[source]['lowest_score'] = min(best_sources[source]['lowest_score'], score)  # Update the lowest score\n",
    "            best_sources[source]['number_of_snippets'] += 1  # Increment the number of snippets for this source\n",
    "\n",
    "    # Sort the sources based on the lowest score\n",
    "    document_titles_unique = sorted(best_sources.items(), key=lambda x: x[1]['lowest_score'])\n",
    "\n",
    "    # Create the document_text using the unique titles (you may need to handle 'title' lookup or fetching more data)\n",
    "    document_text = \"\\n\\n\".join([conditions[title] for title, data in document_titles_unique])\n",
    "\n",
    "    # Prepare the prompt for getting the response\n",
    "    prompt = make_prompt(symptoms_description, document_titles_unique, document_text, k=k)\n",
    "    response = get_response_gpt4(prompt, \"gpt-4o\")\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "cutoff = 1000\n",
    "\n",
    "for k in [5,3]:\n",
    "    ct = 0\n",
    "    tot = 0\n",
    "    for line in tqdm.tqdm(data[:cutoff]):\n",
    "        try:\n",
    "            reranked_documents = re_rank_documents(line, k).split(\",\")\n",
    "            reranked_documents = [x.strip().replace('\"',\"\").replace(\"'\",\"\").replace(\" \",\"-\").lower() for x in reranked_documents]\n",
    "            gold_document = line['conditions_title']\n",
    "            tot += 1\n",
    "            if gold_document in reranked_documents:\n",
    "                ct += 1\n",
    "            if tot % 50 == 0:\n",
    "                print (f\"k={k}, Current accuracy: {ct/tot}\")\n",
    "        except Exception as e:\n",
    "            #print (f\"Error: {e}\")\n",
    "            continue\n",
    "    print (f\"k={k}, Reranking accuracy: {ct/tot}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
