{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53225c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename= \"../data/evaluation/evaluation_vector_store_results_2025-04-26_12-03-28.jsonl\" #k=30\n",
    "#filename = \"../data/evaluation/evaluation_vector_store_results_2025-04-26_12-11-06.jsonl\" #k=50\n",
    "initial_k = 30\n",
    "with open(filename, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b8c6695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['general_demographics', 'symptoms_description', 'query_type', 'severity_level', 'conditions_title', 'query_field', 'target_document_field', 'k', 'retrieved_documents', 'retrieved_documents_scores', 'retrieved_documents_sources', 'match'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_performance = sum([x[\"match\"] for x in data])/len(data)\n",
    "print(f\"Retriever performance: {retriever_performance}\")\n",
    "\n",
    "query_types = set([x[\"query_type\"] for x in data])\n",
    "for query_type in query_types:\n",
    "    query_type_data = [x for x in data if x[\"query_type\"] == query_type]\n",
    "    retriever_performance = sum([x[\"match\"] for x in query_type_data])/len(query_type_data)\n",
    "    print(f\"Retriever performance for {query_type}: {retriever_performance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da126efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Specify the path to your .env file\n",
    "dotenv_path = Path(\"../.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "def get_env_var(key: str) -> str:\n",
    "    try:\n",
    "        return os.environ[key]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Please set the {key} environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d480b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(symptoms_description, document_titles, document_text, k):\n",
    "    prompt = f\"\"\"\n",
    "    You are part of a retrieval system for a medical domain.\n",
    "    Given a description of symptoms provided by a patient, an initial retriever has shortlisted several possible conditions, along with the distance score of the most relevant snippet for that condition (so lower is better), the number of snippets retrieved and the entire content of the associated document.\n",
    "    \n",
    "    Here is the patient's symptom description:\n",
    "    {symptoms_description}\n",
    "    \n",
    "    The shortlisted conditions and their retrieval scores and number of snippets are:\n",
    "    {document_titles}\n",
    "    \n",
    "    The corresponding condition descriptions are:\n",
    "    {document_text}\n",
    "    \n",
    "    Your task is to select the {k} most likely conditions based on the symptoms. \n",
    "    Please return only the titles of the selected conditions, comma-separated.\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_TOKENS = 2048\n",
    "AZURE_OPENAI_API_VERSION = \"2024-12-01-preview\"\n",
    "\n",
    "\n",
    "def get_response_openai(prompt: str, model: str) -> str:\n",
    "    endpoint = get_env_var(f\"AZURE_OPENAI_ENDPOINT_{model}\")\n",
    "    key = get_env_var(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "            api_version=AZURE_OPENAI_API_VERSION,\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=key,\n",
    "        )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful assistant.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                },\n",
    "            ],\n",
    "            max_completion_tokens=MAX_TOKENS,\n",
    "            model=model,\n",
    "        )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_response_openai(\"test\", \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/nhs-conditions/v3/conditions.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "conditions = {}\n",
    "for line in lines:\n",
    "    condition = json.loads(line)\n",
    "    conditions[condition[\"condition_title\"]] = condition['condition_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fa4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_rank_documents(line, k):\n",
    "    symptoms_description = line['symptoms_description']\n",
    "    # document_titles now contains tuples of (source, score)\n",
    "    document_titles = [(source, score) for source, score in zip(line[\"retrieved_documents_sources\"], line[\"retrieved_documents_scores\"])]\n",
    "\n",
    "    # Initialize a dictionary to store best sources with additional data\n",
    "    best_sources = {}\n",
    "\n",
    "    for source, score in document_titles:\n",
    "        # If the source is not in best_sources or the current score is lower than the stored one\n",
    "        if source not in best_sources:\n",
    "            best_sources[source] = {'lowest_score': score, 'number_of_snippets': 1}  # Initialize the source with a count of 1 snippet\n",
    "        else:\n",
    "            best_sources[source]['lowest_score'] = min(best_sources[source]['lowest_score'], score)  # Update the lowest score\n",
    "            best_sources[source]['number_of_snippets'] += 1  # Increment the number of snippets for this source\n",
    "\n",
    "    # Sort the sources based on the lowest score\n",
    "    document_titles_unique = sorted(best_sources.items(), key=lambda x: x[1]['lowest_score'])\n",
    "\n",
    "    # Create the document_text using the unique titles (you may need to handle 'title' lookup or fetching more data)\n",
    "    document_text = \"\\n\\n\".join([conditions[title] for title, data in document_titles_unique])\n",
    "\n",
    "    # Prepare the prompt for getting the response\n",
    "    prompt = make_prompt(symptoms_description, document_titles_unique, document_text, k=k)\n",
    "    response = get_response_openai(prompt, \"gpt-4o\")\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "cutoff = 100\n",
    "\n",
    "reranked_results = []\n",
    "\n",
    "ct = 0\n",
    "tot = 0\n",
    "\n",
    "k = 5\n",
    "\n",
    "for line in tqdm.tqdm(data[:cutoff]):\n",
    "    try:\n",
    "        reranked_documents = re_rank_documents(line, k).split(\",\")\n",
    "        reranked_documents = [x.strip().replace('\"',\"\").replace(\"'\",\"\").replace(\" \",\"-\").lower() for x in reranked_documents]\n",
    "        gold_document = line['conditions_title']\n",
    "        line['reranked_documents'] = reranked_documents\n",
    "        tot += 1\n",
    "        if gold_document in reranked_documents:\n",
    "            line['correct_rerank'] = True\n",
    "            ct += 1\n",
    "        else:\n",
    "            line['correct_rerank'] = False\n",
    "        reranked_results.append(line)\n",
    "        if tot % 50 == 0:\n",
    "            print (f\"Current accuracy: {ct/tot}, initial_k={initial_k}\")\n",
    "    except Exception as e:\n",
    "        print (f\"Error: {e}\")\n",
    "        continue\n",
    "print (f\"Reranking accuracy: {ct/tot}, initial_k={initial_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfadaa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_eval_prompt = open(\"../templates/rag_evaluation_prompt.txt\").read()\n",
    "def make_eval_prompt(symptoms_description, document_titles, demographics, context):\n",
    "    prompt = template_eval_prompt.format(\n",
    "        question=symptoms_description,\n",
    "        sources=document_titles,\n",
    "        demographics=demographics,\n",
    "        context=context\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14df279",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "\n",
    "for result in tqdm.tqdm(reranked_results):\n",
    "    symptoms_description = result['symptoms_description']\n",
    "    demographics = result['general_demographics']\n",
    "    document_titles = result['reranked_documents']\n",
    "    context = \"\\n\\n\".join([conditions[title] for title in document_titles if title in conditions])\n",
    "    \n",
    "    prompt = make_eval_prompt(symptoms_description, document_titles, demographics, context)\n",
    "    response = get_response_openai(prompt, \"o3-mini\")\n",
    "    result['prediction'] = response\n",
    "    final_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_condition_predictions = 0\n",
    "correct_severity_predictions = 0\n",
    "for result in final_results:\n",
    "    gold_condition = result['conditions_title'].lower()\n",
    "    gold_severity = result['severity_level'].lower()\n",
    "    try:\n",
    "        predicted_condition = result['prediction'].split(\",\")[0].strip().replace('\"',\"\").replace(\"'\",\"\").replace(\" \",\"-\").lower()\n",
    "        predicted_severity = result['prediction'].split(\",\")[1].strip().replace('\"',\"\").replace(\"'\",\"\").replace(\" \",\"-\").lower()\n",
    "        if gold_condition == predicted_condition:\n",
    "            result['correct_condition'] = True\n",
    "            correct_condition_predictions += 1\n",
    "        else:\n",
    "            result['correct_condition'] = False\n",
    "        if gold_severity == predicted_severity:\n",
    "            result['correct_severity'] = True\n",
    "            correct_severity_predictions += 1\n",
    "        else:\n",
    "            result['correct_severity'] = False\n",
    "    except Exception as e:\n",
    "        print (result['prediction'])\n",
    "print (f\"Condition prediction accuracy: {correct_condition_predictions/len(final_results)}\")\n",
    "print (f\"Severity prediction accuracy: {correct_severity_predictions/len(final_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_type in query_types:\n",
    "    query_type_data = [x for x in final_results if x[\"query_type\"] == query_type]\n",
    "    correct_condition_predictions = sum([x[\"correct_condition\"] for x in query_type_data if \"correct_condition\" in x])\n",
    "    correct_severity_predictions = sum([x[\"correct_severity\"] for x in query_type_data if \"correct_severity\" in x])\n",
    "    print (f\"Condition prediction accuracy for {query_type}: {correct_condition_predictions/len(query_type_data)}\")\n",
    "    print (f\"Severity prediction accuracy for {query_type}: {correct_severity_predictions/len(query_type_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the final results\n",
    "import json\n",
    "with open(f\"../data/evaluation/evaluation_vector_store_results_2025-04-26_12-03-28_reranked_predicted.jsonl\", 'w') as f:\n",
    "    for result in final_results:\n",
    "        f.write(json.dumps(result) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
