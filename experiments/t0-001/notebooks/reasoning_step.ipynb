{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# given the output of our RAG system\n",
    "with open(\"../data/evaluation_results_2025-04-04_11-08-43.jsonl\", \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a patient with symptoms\n",
    "query = data[0]\n",
    "\n",
    "# Information about the patient\n",
    "general_demographics = query[\"general_demographics\"]\n",
    "symptoms_description = query[\"symptoms_description\"]\n",
    "\n",
    "# Informationa about the chunks retrieved (to be used when reasoning)\n",
    "k = query[\"k\"]\n",
    "retrieved_chunks = query[\"retrieved_documents\"]\n",
    "retrieved_documents_scores = query[\"retrieved_documents_scores\"]\n",
    "retrieved_documents_sources = query[\"retrieved_documents_sources\"]\n",
    "\n",
    "# Grouping information about the chunks retrieved by the potential condition\n",
    "potential_conditions = {x: [] for x in set(retrieved_documents_sources)}\n",
    "\n",
    "for i in range(len(retrieved_documents_sources)):\n",
    "    potential_conditions[retrieved_documents_sources[i]].append((retrieved_chunks[i]))\n",
    "\n",
    "print(potential_conditions.keys())\n",
    "\n",
    "# And also information for measuring performance (not to use when querying the model)\n",
    "query_type = query[\"query_type\"]\n",
    "severity_level = query[\"severity_level\"]\n",
    "conditions_title = query[\"conditions_title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def get_response_from_model(prompt, model=\"gemma3:1b\"):\n",
    "    response = ollama.generate(model=model, prompt=prompt)\n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "Given this symptom description: \"{symptoms_description}\".\n",
    "And the following information about the patient: {general_demographics}.\n",
    "Our system suggests one of the following conditions: {\", \".join(potential_conditions.keys())}.\n",
    "\n",
    "Decide which of the following conditions is the most likely and the level of severity among the following options:\n",
    "\n",
    "### Severity Level:\n",
    "* **Not urgent**: Monitor at home\n",
    "* **Medium**: Go to the GP\n",
    "* **Medium urgent**: Get an urgent GP appointment\n",
    "* **Urgent**: Call 999\n",
    "\n",
    "For each potential condition, we provide the most relevant information from the retrieved documents:\n",
    "\n",
    "{json.dumps(potential_conditions, indent=2)}\n",
    "\n",
    "Reply only with the most likely condition and the severity level, in the following format:\n",
    "\n",
    "### Condition: <condition>\n",
    "### Severity Level: <severity_level>\n",
    "\n",
    "If you think that the condition is not listed, please say \"None of the above\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response_from_model(prompt_template, model=\"deepseek-r1:8b\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
