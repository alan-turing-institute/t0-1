```
uv run t0-001 evaluate-vector-store \
  data/synthetic_queries/5bb7345_gpt-4o_1000_synthetic_queries.jsonl \
  --k <k> \
  --distance-metric (either "cosine" or "l2")
```

Currently our chunk size is the size of the context length of the embedding model. For long context models, we might want to set a hard limit to the context length of the chunks like to `min(context_length_of_embedding_model, 512)`.

| Embedding Method | Dims | Context length | Chunks in DB | DB | Metric | p@1 | p@5 | p@10 | p@20 | Notes |
|-|-|-|-|-|-:|-:|-:|-:|-|
| all-mpnet-base-v2 | 768 | 384 | 5834 | Chroma | Squared L2 norm | 0.423 | 0.624 | 0.708 | 0.787 | - |
| all-mpnet-base-v2 | 768 | 384 | 5834 | Chroma | Cosine | - | - | - | - | - |
| all-distilroberta-v1 | 768 | 512 | 4707 | Chroma | Squared L2 norm | 0.40 | 0.59 | 0.68 | 0.75 | - |
| intfloat/multilingual-e5-large-instruct | 1024 | 512 | 4667 | Chroma | Squared L2 norm | 0.37 | 0.61 | 0.70 | 0.77 | Need to add instructions to the query. For querying, I prepended "Retrieve semantically similar text. Query:" to the query which increased performance very slightly. |
| intfloat/multilingual-e5-large | 1024 | 512 | 4667 | Chroma | Squared L2 norm | - | - | - | - | - |
| Alibaba-NLP/gte-modernbert-base | 768 | 8192 | - | Chroma | Squared L2 norm | - | - | - | - | - |
| BAAI/bge-m3 | 1024 | 8192 | - | Chroma | Squared L2 norm | - | - | - | - | - |
| BAAI/bge-large-en-v1.5 | 1024 | 512 | 4353 | Chroma | Squared L2 norm | - | - | - | - | - |
